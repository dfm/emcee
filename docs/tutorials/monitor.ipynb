{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(monitor)=\n",
    "\n",
    "# Saving & monitoring progress\n",
    "\n",
    "It is often useful to incrementally save the state of the chain to a file.\n",
    "This makes it easier to monitor the chainâ€™s progress and it makes things a little less disastrous if your code/computer crashes somewhere in the middle of an expensive MCMC run.\n",
    "\n",
    "In this demo, we will demonstrate how you can use the new {class}`backends.HDFBackend` to save your results to a [HDF5](https://en.wikipedia.org/wiki/Hierarchical_Data_Format) file as the chain runs.\n",
    "To execute this, you'll first need to install the [h5py library](http://www.h5py.org).\n",
    "\n",
    "We'll also monitor the autocorrelation time at regular intervals (see {ref}`autocorr`) to judge convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = \"retina\"\n",
    "\n",
    "from matplotlib import rcParams\n",
    "rcParams[\"savefig.dpi\"] = 100\n",
    "rcParams[\"figure.dpi\"] = 100\n",
    "rcParams[\"font.size\"] = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set up the problem as usual with one small change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# The definition of the log probability function\n",
    "# We'll also use the \"blobs\" feature to track the \"log prior\" for each step\n",
    "def log_prob(theta):\n",
    "    log_prior = -0.5 * np.sum((theta - 1.0) ** 2 / 100.0)\n",
    "    log_prob = -0.5 * np.sum(theta ** 2) + log_prior\n",
    "    return log_prob, log_prior\n",
    "\n",
    "\n",
    "# Initialize the walkers\n",
    "coords = np.random.randn(32, 5)\n",
    "nwalkers, ndim = coords.shape\n",
    "\n",
    "# Set up the backend\n",
    "# Don't forget to clear it in case the file already exists\n",
    "filename = \"tutorial.h5\"\n",
    "backend = emcee.backends.HDFBackend(filename)\n",
    "backend.reset(nwalkers, ndim)\n",
    "\n",
    "# Initialize the sampler\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_prob, backend=backend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference here was the addition of a \"backend\".\n",
    "This choice will save the samples to a file called `tutorial.h5` in the current directory.\n",
    "Now, we'll run the chain for up to 10,000 steps and check the autocorrelation time every 100 steps.\n",
    "If the chain is longer than 100 times the estimated autocorrelation time and if this estimate changed by less than 1%, we'll consider things converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_n = 100000\n",
    "\n",
    "# We'll track how the average autocorrelation time estimate changes\n",
    "index = 0\n",
    "autocorr = np.empty(max_n)\n",
    "\n",
    "# This will be useful to testing convergence\n",
    "old_tau = np.inf\n",
    "\n",
    "# Now we'll sample for up to max_n steps\n",
    "for sample in sampler.sample(coords, iterations=max_n, progress=True):\n",
    "    # Only check convergence every 100 steps\n",
    "    if sampler.iteration % 100:\n",
    "        continue\n",
    "\n",
    "    # Compute the autocorrelation time so far\n",
    "    # Using tol=0 means that we'll always get an estimate even\n",
    "    # if it isn't trustworthy\n",
    "    tau = sampler.get_autocorr_time(tol=0)\n",
    "    autocorr[index] = np.mean(tau)\n",
    "    index += 1\n",
    "\n",
    "    # Check convergence\n",
    "    converged = np.all(tau * 100 < sampler.iteration)\n",
    "    converged &= np.all(np.abs(old_tau - tau) / tau < 0.01)\n",
    "    if converged:\n",
    "        break\n",
    "    old_tau = tau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at how the autocorrelation time estimate (averaged across dimensions) changed over the course of this run.\n",
    "In this plot, the $\\tau$ estimate is plotted (in blue) as a function of chain length and, for comparison, the $N > 100\\,\\tau$ threshold is plotted as a dashed line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 100 * np.arange(1, index + 1)\n",
    "y = autocorr[:index]\n",
    "plt.plot(n, n / 100.0, \"--k\")\n",
    "plt.plot(n, y)\n",
    "plt.xlim(0, n.max())\n",
    "plt.ylim(0, y.max() + 0.1 * (y.max() - y.min()))\n",
    "plt.xlabel(\"number of steps\")\n",
    "plt.ylabel(r\"mean $\\hat{\\tau}$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we can also access all the properties of the chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "\n",
    "tau = sampler.get_autocorr_time()\n",
    "burnin = int(2 * np.max(tau))\n",
    "thin = int(0.5 * np.min(tau))\n",
    "samples = sampler.get_chain(discard=burnin, flat=True, thin=thin)\n",
    "log_prob_samples = sampler.get_log_prob(discard=burnin, flat=True, thin=thin)\n",
    "log_prior_samples = sampler.get_blobs(discard=burnin, flat=True, thin=thin)\n",
    "\n",
    "print(\"burn-in: {0}\".format(burnin))\n",
    "print(\"thin: {0}\".format(thin))\n",
    "print(\"flat chain shape: {0}\".format(samples.shape))\n",
    "print(\"flat log prob shape: {0}\".format(log_prob_samples.shape))\n",
    "print(\"flat log prior shape: {0}\".format(log_prior_samples.shape))\n",
    "\n",
    "all_samples = np.concatenate(\n",
    "    (samples, log_prob_samples[:, None], log_prior_samples[:, None]), axis=1\n",
    ")\n",
    "\n",
    "labels = list(map(r\"$\\theta_{{{0}}}$\".format, range(1, ndim + 1)))\n",
    "labels += [\"log prob\", \"log prior\"]\n",
    "\n",
    "corner.corner(all_samples, labels=labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, since you saved your samples to a file, you can also open them after the fact using the {class}`backends.HDFBackend`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = emcee.backends.HDFBackend(filename)\n",
    "\n",
    "tau = reader.get_autocorr_time()\n",
    "burnin = int(2 * np.max(tau))\n",
    "thin = int(0.5 * np.min(tau))\n",
    "samples = reader.get_chain(discard=burnin, flat=True, thin=thin)\n",
    "log_prob_samples = reader.get_log_prob(discard=burnin, flat=True, thin=thin)\n",
    "log_prior_samples = reader.get_blobs(discard=burnin, flat=True, thin=thin)\n",
    "\n",
    "print(\"burn-in: {0}\".format(burnin))\n",
    "print(\"thin: {0}\".format(thin))\n",
    "print(\"flat chain shape: {0}\".format(samples.shape))\n",
    "print(\"flat log prob shape: {0}\".format(log_prob_samples.shape))\n",
    "print(\"flat log prior shape: {0}\".format(log_prior_samples.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should give the same output as the previous code block, but you'll notice that there was no reference to `sampler` here at all.\n",
    "\n",
    "If you want to restart from the last sample, you can just leave out the call to {func}`backends.HDFBackend.reset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_backend = emcee.backends.HDFBackend(filename)\n",
    "print(\"Initial size: {0}\".format(new_backend.iteration))\n",
    "new_sampler = emcee.EnsembleSampler(nwalkers, ndim, log_prob, backend=new_backend)\n",
    "new_sampler.run_mcmc(None, 100)\n",
    "print(\"Final size: {0}\".format(new_backend.iteration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to save *additional* ``emcee`` runs, you can do so on the same file\n",
    "as long as you set the ``name`` of the backend object to something other than\n",
    "the default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run2_backend = emcee.backends.HDFBackend(filename, name=\"mcmc_second_prior\")\n",
    "\n",
    "# this time, with a subtly different prior\n",
    "def log_prob2(theta):\n",
    "    log_prior = -0.5 * np.sum((theta - 2.0) ** 2 / 100.0)\n",
    "    log_prob = -0.5 * np.sum(theta ** 2) + log_prior\n",
    "    return log_prob, log_prior\n",
    "\n",
    "\n",
    "# Rinse, Wash, and Repeat as above\n",
    "coords = np.random.randn(32, 5)\n",
    "nwalkers, ndim = coords.shape\n",
    "sampler2 = emcee.EnsembleSampler(nwalkers, ndim, log_prob2, backend=run2_backend)\n",
    "\n",
    "# note: this is *not* necessarily the right number of iterations for this\n",
    "# new prior.  But it will suffice  to demonstrate the second backend.\n",
    "sampler2.run_mcmc(coords, new_backend.iteration, progress=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now you can see *both* runs are in the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File(filename, \"r\") as f:\n",
    "    print(list(f.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
